{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthkodnani92/parthkodnani92-HR-Analytics-to-Predict-Employee-Turnover/blob/main/FinalGroup21_BUDT704_FlyingSquirrel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1drMrmIKRtt"
      },
      "source": [
        "# Project Flying Squirrel: Investigating the Factors That Affect Job Change Rate\n",
        "\n",
        "## BUDT 704 Group 21 \n",
        "\n",
        "###  Atharva Patil, Javan Reuto, Parth Kodnani, Snigdha Madiraju, Sumeet Ram, Yingsheng Lin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EBeTd7bKRt5"
      },
      "source": [
        "### Project Overview: \n",
        "\n",
        "The dataset that we chose to analyze is called “HR Analytics: Job Change of Data Scientists” and was retrieved from: https://www.kaggle.com/uniabhi/hr-analytics-job-change-of-data-scientists. \n",
        "\n",
        "This dataset is centered around the fact that a company, who is interested in Big Data Analytics, wants to conduct training and provide relevant courses to equip their future employees. However, due to high costs and the large amount of time these trainings require, the company is further interested in identifying the real retention rate of these employees. \n",
        "\n",
        "The dataset covers different factors (characterized by the different columns) that may affect a person’s decision to leave their current job. Using this information, the company is interested in looking at how those factors might translate to an employee leaving the company and how it can improve the training programs to increase employee retention rate. \n",
        "\n",
        "This analysis is especially relevant to us, those in our age group, and those who are looking to enter the workforce. It not only helps us understand what factors affect an employee’s decision to leave a company, but also allows us to be better prepared for the future when we potentially face such circumstances. This dataset can be applicable to anyone, regardless of what field/industry they come from, as the insights are universal. \n",
        "\n",
        "The questions we are looking to answer through this analysis are as follows: \n",
        "\n",
        "1.  How can the company predict the probability of employees wanting to work with companies by training models?\n",
        "\n",
        "2.  What are the factors that affect an employee's decision to stay with/leaving a company?\n",
        "\n",
        "Both of these questions are answered through modeling and data visualization, emphasizing our utilization of thorough data analysis to bring recommendations to the company of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxGmcl9MNvpC"
      },
      "source": [
        "### Data Analysis vs. Data Processing\n",
        "\n",
        "The dataset was based on a target for job retention for employees looking to either stay in their current job or move to a different job. The main data processing task that required beyond basic processing was the encoding of categorical variables for models that were performed in our analysis. For this reason, we chose to focus heavily on data analysis. \n",
        "\n",
        "The visualization was focused on the variables that potentially contributed to an employee either looking for a job change or not. Examples of such visualization include: a scatterplot for city development index, correlation of features, and bar plots covering multiple categories. The analysis that goes beyond basic analysis include Decision Tree, Random Forest, LGBM, Logistic Regression, and KNN models. These models serve the purpose of predicting the column variable \"Target”, which represents whether an employee is looking for a job change or not. This goes beyond basic visualization because it incorporates machine learning algorithms that allow us to build a prediction on a \"test\" data set. The prediction itself is based on the accuracy of the model created, with the prediction serving as the \"Target\" column. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiyJdLsKRuD"
      },
      "source": [
        "### Data Processing: \n",
        "Data Processing is \"the collection and manipulation of items of data to produce meaningful information\". The given dataset is very imbalanced and most features are categorical (Nominal, Ordinal, Binary), some with high cardinality. \n",
        "\n",
        "In order to successfully analyze the data, we first needed to process the data to prepare it for analysis. The following tasks were performed to clean/transform the dataset:\n",
        "\n",
        "\n",
        "* Bin City Development Index\n",
        "\n",
        "* Drop duplicate values for Experience and Relevant Experience \n",
        "\n",
        "* Fill NaN values with “other” for Experience and Relevant Experience\n",
        " \n",
        "* Create crosstabs for Experience and Relevant Experience\n",
        " \n",
        "* Sort number of candidates who change jobs by descending order\n",
        " \n",
        "* Bin Experience \n",
        " \n",
        "* Impute null values with “NA” for Enrolled University & Education Level \n",
        " \n",
        "* Drop duplicate values for Gender & Major Discipline Columns\n",
        " \n",
        "* Fill NaN values with “other” for Gender & Major Discipline Columns\n",
        " \n",
        "* Create crosstabs for Gender & Major Discipline Columns\n",
        " \n",
        "* Replace null values with “not known” for Company Size & Company Type \n",
        " \n",
        "* Find frequency of each category in Company Type \n",
        " \n",
        "* Adjust “never” to “0” for Last New Job & Training Hours\n",
        " \n",
        "* Bin Training Hours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynQrpt9uKRuD"
      },
      "source": [
        "#### Import Libraries: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9hb6x2uKRuE"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgbm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDumyMYC2w0W"
      },
      "source": [
        "#### 1. City & City Development Index Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYYMYkZVCjzh"
      },
      "source": [
        "Read data from csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-I_O5lr8eoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "5af333a0-f2e1-4cc7-f5e6-38efbf592765"
      },
      "source": [
        "aug_traindf=pd.read_csv(r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-33b6c3a86c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maug_traindf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\linyi\\Desktop\\aug_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\linyi\\\\Desktop\\\\aug_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZa-YQIS0tM6"
      },
      "source": [
        "Binning of the City Development Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH82doU20tM7"
      },
      "source": [
        "bins_city_development_index = np.linspace(start=0.000, stop=1.0, num=11) # specifies the number of bins to be 10 with 1 being the max city development index value\n",
        "aug_traindf['city_development_index_bins'] = pd.cut(aug_traindf['city_development_index'],bins_city_development_index) \n",
        "bin_count = aug_traindf['city_development_index_bins'].value_counts().sort_index() # sorts the bins from beginning to end to represent the frequency within each bin\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_FR2Ig0tM7"
      },
      "source": [
        "City Development Index Bin Variable Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfmXyDvy0tM8"
      },
      "source": [
        "bins1 = bin_count.index.tolist() # variable of bin categories created as a list for use in barplot visualizations\n",
        "bin_count_list = (list(bin_count)) # variable of bin frequency created as a list for use in barplot visualizations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BurJPT3AKRuG"
      },
      "source": [
        "#### 2. Relevant Experience & Experience Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMxglPFIKRuH"
      },
      "source": [
        "Read the data from csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gude7bhtKRuH"
      },
      "source": [
        "df=pd.read_csv(r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\") # path needs to be changed when file path changed\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNYkUBa3616w"
      },
      "source": [
        "Adjust duplicate and null values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkF9Hm0iKRuH"
      },
      "source": [
        "df.drop_duplicates(keep = False) #drop all the columns that have duplicated values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW9ShE4dKRuI"
      },
      "source": [
        "df.isnull().sum(axis=0) # sum the null values of all columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG5_0bOBKRuI"
      },
      "source": [
        "df1=df.fillna('Other') # fill the nan values with 'Other'\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4OOmTgFKRuI"
      },
      "source": [
        "Find the unique elements in columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzjykLh6KRuJ"
      },
      "source": [
        "relevantexperiencetype=df1['relevent_experience'].unique() # find the unique elements in the column of 'relevent_experience'\n",
        "relevantexperiencetype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VJ2u1fuKRuJ"
      },
      "source": [
        "experiencetype=df1['experience'].unique() # find the unique elements in the column of 'relevent_experience'\n",
        "experiencetype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65JomDBXKRuJ"
      },
      "source": [
        "Draw frequency table of relevant experience by crosstab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3oQ6xJJKRuJ"
      },
      "source": [
        "releventexperiencefreq=pd.crosstab(index=df1['relevent_experience'], columns='count')# use crosstab to count the elements in 'relevent_experience' column\n",
        "releventexperiencefreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LZdiv6KRuK"
      },
      "source": [
        "Draw bar plots of relevant experience column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtgpx6FyKRuK"
      },
      "source": [
        "object=('Has relevent experience','No relevent experience') # name of the columns\n",
        "value=[13748,5345] #number of values\n",
        "width=0.5\n",
        "plt.barh(object,value) # plot bar graph\n",
        "for index, num in enumerate(value):  # add number labels to the bar plot\n",
        "    plt.text(num, index,str(num))\n",
        "plt.title(\"Bar plots of relevent experience conduction\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKsnEIEIKRuK"
      },
      "source": [
        "Create frequency table by using crosstab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvZ8NIJmKRuK"
      },
      "source": [
        "experiencefreq=pd.crosstab(index=df1['experience'], columns='count') # use crosstab to count the elements in 'experience' column\n",
        "experiencefreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJEyPQWKKRuL"
      },
      "source": [
        "Create bins for the 'experience' column, \n",
        "\n",
        "Replace categories '<1' , '>20' with numeric values to process bin count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8P4Jp4aKRuL"
      },
      "source": [
        "df2=df1['experience'].replace('<1',0) # replace <1 with 0 as bins are for numeric values\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jq6jTDCKRuL"
      },
      "source": [
        "df2=df2.replace('>20',float('inf')) # replace >20 with inf as bins are for numeric values\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eh8kiYp0tNH"
      },
      "source": [
        "df2.drop(df2.index[df2 == 'Other'], inplace = True)#drop \"Other' to conduct bins. 'other' are the nan values\n",
        "df2=pd.to_numeric(df2)# change df2 to numeric to process "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gug27zf8KRuL"
      },
      "source": [
        "bins = [float('-inf'),0,3,6,9,12,15,18,float('inf') ]\n",
        "classify = pd.cut(df2,bins) # cut df2 based on the bins\n",
        "df3=pd.value_counts(classify).sort_index() # count the values and sort the index by ascending order\n",
        "df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjBq2tUoKRuM"
      },
      "source": [
        "Create frequency tables by using crosstab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afFSnslEKRuM"
      },
      "source": [
        "table=pd.crosstab(df1.relevent_experience, df1.target) # use crosstab to count the elements in 'relevent_experience' column\n",
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOaEDB5yKRuM"
      },
      "source": [
        "Use crosstab to calculate the frequency of the elements in column 'experience' and column 'target', sort the number of candidates who change jobs in descending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crb9ackmKRuN"
      },
      "source": [
        "table1=pd.crosstab(df1.experience, df1.target) # use crosstab to count the elements in 'experience' column and 'target' column\n",
        "table1=table1.sort_values(by=[1.0],ascending=False) # sort the value in descending order\n",
        "table1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3EOZA6UKRuN"
      },
      "source": [
        "#### 3. Enrolled University & Education Level Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkcE4p3OfF0"
      },
      "source": [
        "Description of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwMBBS_dORar"
      },
      "source": [
        "# describing the data to see its characteristics\n",
        "print(f'Rows, Columns of DataFrame: {df.shape}')\n",
        "print(f'Total values of DataFrame: {df.size}')\n",
        "print(f'Rows of DataFrame: {len(df)}')\n",
        "print(f'Columns of DataFrame: {len(df.columns)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCWu5ssCOjkv"
      },
      "source": [
        "Data Cleaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIr1uvEPOp3J"
      },
      "source": [
        "We see quite a lot of null values. Most of them are present in the categorical variable columns. Hence, we replace them with the most common observation; the mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rA47ymZOisN"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x8FmzV2OxOj"
      },
      "source": [
        "df.isnull().sum() # find all the nan values in the data frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6pSqQtrO0gt"
      },
      "source": [
        "dfEducation = df[['major_discipline', 'enrolled_university', 'education_level', 'target']] # get 4 columns out of the dataframe\n",
        "dfEducation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iLw9f-rO262"
      },
      "source": [
        "null = dfEducation.isnull() # To recognize null values\n",
        "nullAny = null.any(axis = 1) # To check for any null values\n",
        "nullPrint = dfEducation[nullAny] # To put these null values in a dataframe\n",
        "nullPrint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNAyS92POg36"
      },
      "source": [
        "dfEducation = dfEducation.fillna(\"NA\") # To impute all values with NA\n",
        "dfEducation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0AEc03tPKMW"
      },
      "source": [
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDZMlBw5PApq"
      },
      "source": [
        "- We check the number of categories every variable has.\n",
        "- We want to analyze the number of people in these categories and whether they want to leave the company or stay with the company.\n",
        "- We plot these categories as bar charts to clearly visualize the number of people in these categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx4WZabeCOec"
      },
      "source": [
        "catEd = dfEducation['education_level'].unique() # check the type of different education level\n",
        "catEd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwxB6EONCPW7"
      },
      "source": [
        "ptableEdu = pd.crosstab(dfEducation['education_level'], dfEducation['target'], rownames = ['education_level'], colnames = ['target']).sort_values(by = 0, ascending = False) # crosstab shows the number of job changing and job does not change for different major-disciplines\n",
        "ptableEdu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLNF6jja0tNO"
      },
      "source": [
        "dfEducation3 = dfEducation[['education_level', 'target']]\n",
        "sns.countplot(x = 'education_level', hue = 'target', data = dfEducation3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp1lcHYjQ0wt"
      },
      "source": [
        "catUni = dfEducation['enrolled_university'].unique() # check the categories of enrolled university \n",
        "catEd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSjOYHI0tNO"
      },
      "source": [
        "enrolleduniversity = pd.crosstab(dfEducation['enrolled_university'], dfEducation['target'], rownames = ['enrolled_univerity'], colnames = ['target']).sort_values(by = 0, ascending = False) # pivot table to show the distribution of people enrolled in different types of university\n",
        "enrolleduniversity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgst7efQF2x"
      },
      "source": [
        "dfEducation4 = dfEducation[['enrolled_university', 'target']]\n",
        "sns.countplot(x = 'education_level', hue = 'target', data = dfEducation4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL5xXOhUKRuN"
      },
      "source": [
        "#### 4. Gender & Major Discipline Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copAOGIZKRuO"
      },
      "source": [
        "Clean up data by dropping duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiNcCoWQKRuO"
      },
      "source": [
        "df.drop_duplicates(keep = False) # drop all the duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COKuu6W8KRuO"
      },
      "source": [
        "df.isnull().sum(axis=0) # find null values in the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qKOH4RoKRuO"
      },
      "source": [
        "df1=df.fillna('Other') # Fill the nan values with 'Other'\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEBUT3h8KRuO"
      },
      "source": [
        "gendercol = df1['gender']\n",
        "gender = gendercol.unique() # find out the types of gender in the 'gender' column\n",
        "gender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoym8Ig5B2Ju"
      },
      "source": [
        "ptableUni = pd.crosstab(dfEducation['major_discipline'], dfEducation['target'], rownames = ['major_discipline'], colnames = ['target']).sort_values(by = 0, ascending = False) # pivot table to show the distribution of people of various majors\n",
        "ptableUni"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e94PeS1e0tNQ"
      },
      "source": [
        "dfEducation2 = dfEducation[['enrolled_university', 'target']]\n",
        "sns.countplot(x = 'enrolled_university', hue = 'target', data = dfEducation2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFbkG1hSRTyT"
      },
      "source": [
        "Create crosstabs for gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9nNCnhAKRuP"
      },
      "source": [
        "gender= pd.crosstab(index = df['gender'], columns = 'count')# find the number of each type of gender in the gender column\n",
        "gender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43aKPV3RY0O"
      },
      "source": [
        "Create crosstabs for major discipline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op2zfQiIKRuP"
      },
      "source": [
        "majorcol = df['major_discipline']\n",
        "majorcol.unique()\n",
        "major = pd.crosstab(index=df['major_discipline'], columns = 'count')#  find the number of each type of major_discipline in the major_discipline column\n",
        "major"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ELX0tOKRuP"
      },
      "source": [
        "#### 5. Company Size & Company Type Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB1slu0YyPQn"
      },
      "source": [
        "Read csv data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwx87EVpKRuP"
      },
      "source": [
        "train_HR=pd.read_csv(r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\")\n",
        "train_HR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXyIPkAEyWeE"
      },
      "source": [
        "Replace missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N5KnJSmyb0G"
      },
      "source": [
        "For the 'Company type' and 'Company size', set the missing values to 'Not Known' because the number of missing values for these two columns are significantly high. Therefore, we treat the category 'Not Known' as a category itself and do analysis on it given that we cannot ignore such a large number of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukuL21W6yavR"
      },
      "source": [
        "#Replacing all the Nan values with 'Not Known' for Company Type\n",
        "train_HR.company_type = train_HR.company_type.fillna('Not Known')\n",
        "train_HR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNGJrKasysAN"
      },
      "source": [
        "#Replacing all the Nan values with 'Not Known' for Company Size\n",
        "train_HR.company_size = train_HR.company_size.fillna('Not Known')\n",
        "train_HR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6xJHOF9yuVn"
      },
      "source": [
        "train_HR.isnull().sum(axis=0) # To check the existence of null values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SU2lwWny0xJ"
      },
      "source": [
        "Identifying the unique values for company_type and company_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfu0wlI7y1dK"
      },
      "source": [
        "train_companytype=train_HR['company_type'].unique()\n",
        "train_companytype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832aMjOXy2-g"
      },
      "source": [
        "train_companysize=train_HR['company_size'].unique()\n",
        "train_companysize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMv7OScOy58o"
      },
      "source": [
        "Calculate frequency of each category and the proportion (percentage) of observations for each category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XYxWmIczFMe"
      },
      "source": [
        "company_type_count = train_HR['company_size'].value_counts(normalize=True)\n",
        "company_type_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj9RxZRp0tNV"
      },
      "source": [
        "company_type_frequency = train_HR['company_size'].value_counts()\n",
        "company_type_frequency\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h1hrnelKRuQ"
      },
      "source": [
        "#### 6. Last New Job & Training Hours Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHmvb1mKRuQ"
      },
      "source": [
        "Add columns into respective data frames "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFoZ-poRKRuQ"
      },
      "source": [
        "#Last New Job from Column 11\n",
        "df5 = pd.read_csv (r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\", usecols= ['enrollee_id','last_new_job'])\n",
        "df5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5mRLzG4KRuQ"
      },
      "source": [
        "#Training Hours from Column 12\n",
        "df6 = pd.read_csv (r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\", usecols= ['enrollee_id','training_hours'])\n",
        "df6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "766ISr7uKRuQ"
      },
      "source": [
        "Replace inaccurate values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVyvzkP1KRuQ"
      },
      "source": [
        "#Change Never to 0\n",
        "df5['last_new_job'].str.replace('never', '0', n=- 1, case=None, flags=0, regex=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Dy4eAhKRuR"
      },
      "source": [
        "Bin Training Hours Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jttmM8ZuKRuR"
      },
      "source": [
        "#Bin Training Hours Column \n",
        "interval_range = pd.interval_range(start=1, freq=50, end=336)\n",
        "df6['cut_training_hours'] = pd.cut(df6['training_hours'], bins=interval_range, labels=[1,2,3,4,5,6,7])\n",
        "df6.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwEuXIJGKRuR"
      },
      "source": [
        "### Data Analysis & Visualizations: \n",
        "Data Analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data Visualization is a process of communicating your data in the form of a graphic representation. It describes the process in an efficient and legible manner.\n",
        "\n",
        "The following data visualizations includes tasks such as creating bar graphs for certain columns, creating histograms for other columns, and creating a scatter plot of one column (see data analysis section for more details). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pky7Z2LAKRuR"
      },
      "source": [
        "#### Create Histogram for City Development Index Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtinLmT-KRuS"
      },
      "source": [
        "title = f'City Development Index'\n",
        "sns.set_style('darkgrid')\n",
        "barplot_city_index_bins = sns.barplot(x = bins1, y = bin_count_list, palette = 'colorblind') # x and y were determined in the processing stage of the city development index\n",
        "\n",
        "barplot_city_index_bins.set_xticklabels(barplot_city_index_bins.get_xticklabels(), rotation=40, ha=\"right\")# the bins are formatted and rotated to be visually appealing and not overlapping \n",
        "# the 4 lines of code below name the x and y axis and displays the data at the top of each bar\n",
        "barplot_city_index_bins.set_title(title)\n",
        "barplot_city_index_bins.set_xlabel('City Development Index Bins')\n",
        "barplot_city_index_bins.set_ylabel('City Development Index Frequency')\n",
        "barplot_city_index_bins.set_ylim(top=max(bin_count_list) * 1.15)\n",
        "# This for loop  purpose is to iterate through the lists created in the processing stage, both lists are of equal length therefore match correctly to each value in the corrosponding list \n",
        "# The text, numbers, and bins are formatted to visually appeal to the reader\n",
        "for bar, bin_count_list in zip(barplot_city_index_bins.patches,bin_count_list):\n",
        "    text_city_index_bins = bar.get_x() + bar.get_width() / 2.0\n",
        "    text_city_index_bins_count = bar.get_height()\n",
        "    text = f'{bin_count_list}'\n",
        "    barplot_city_index_bins.text(text_city_index_bins, text_city_index_bins_count, text, ha='center', va='bottom')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4lDZ2gKKRuS"
      },
      "source": [
        "#### Create Bar Graph for Relevant Experience Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbWw5jJ-KRuS"
      },
      "source": [
        "object=('Has relevent experience','No relevent experience')# name of the columns\n",
        "value=[13748,5345] #number of values\n",
        "width=0.5\n",
        "plt.barh(object,value)# plot bar graph\n",
        "for index, num in enumerate(value):  # add number labels to the bar plot\n",
        "    plt.text(num, index,str(num))\n",
        "plt.title(\"Bar plots of relevent experience conduction\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr3lf6iMKRuS"
      },
      "source": [
        "#### Create Pie Chart for Experience Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCg-4tgLKRuS"
      },
      "source": [
        "labels=['0','0 to 3','3 to 6','6 to 9','9 to 12','12 to 15','15 to 18','18 to infinity'] # creating categories for experience\n",
        "plt.pie(df3,labels=labels) # draw pie chart\n",
        "plt.title(\"Percentage of experience times of candidates\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcjV1ZuzKRuS"
      },
      "source": [
        "#### Calculate Percentage of Job Change Rate for Relevant Experience Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDHhcdSUKRuT"
      },
      "source": [
        "Use groupby to calculate the percentage of job change rate in relevant experience column "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOmu8nnwKRuT"
      },
      "source": [
        "jcrelexperiencepercent=df1.groupby(\"relevent_experience\")[\"target\"].mean() #the percentage of job change rate of elements in column 'relevent_experience'\n",
        "jcrelexperiencepercent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1FTsteKRuT"
      },
      "source": [
        "#### Calculate Percentage of Job Change Rate for Experience Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vP3yy89KRuT"
      },
      "source": [
        "Use groupby to calculate the percentage of job change rate in experience column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeEEASoNKRuT"
      },
      "source": [
        "jcexperiencepercent=df1.groupby(\"experience\")[\"target\"].mean().sort_values(ascending=False) # the percentage of job change rate of elements in column 'experience'\n",
        "jcexperiencepercent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxsqbMNrKRuT"
      },
      "source": [
        "Use groupby to calculate the percentage of job change rate in education level column "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b39MX-ppKRuU"
      },
      "source": [
        "df1.groupby(\"education_level\")['target'].mean().sort_values() # the percentage of job change rate of elements in column 'education_level'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voKDaDan0tNa"
      },
      "source": [
        "Create Bar Graphs for Education Level Columns¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJJRgUhzCGsB"
      },
      "source": [
        "dfEducation3 = dfEducation[['education_level', 'target']] \n",
        "sns.countplot(x = 'education_level', hue = 'target', data = dfEducation3) # Create a bar graph which specifies the target variable for every category of education level"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMpG-H4KRuU"
      },
      "source": [
        "#### Calculate Percentage of Job Change Rate for Enrolled University Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjzqSWBfQONT"
      },
      "source": [
        "Use groupby to calculate the percentage of job change rate in enrolled university column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLMA3BhKRuU"
      },
      "source": [
        "df1.groupby(\"enrolled_university\")['target'].mean().sort_values() # the percentage of job change rate of elements in column 'enrolled_university'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA6lihgrPqdf"
      },
      "source": [
        "#### Create Bar Graphs for Enrolled University\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QBayFv-Puyc"
      },
      "source": [
        "enrolleduniversity.plot(kind='bar', figsize = (7,6), rot = 0);\n",
        "plt.xlabel('Enrolled University', labelpad = 14,fontsize=15)\n",
        "plt.ylabel('Count', labelpad = 14,fontsize=15)\n",
        "plt.title(\"Count by Enrolled University\", y=1.02, fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoCh24GiKRuU"
      },
      "source": [
        "#### Create Bar Graphs for Gender & Major Discipline Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH82y0ScKRuV"
      },
      "source": [
        "gender.plot(kind='bar', figsize = (7,6), rot = 0);\n",
        "plt.xlabel('Gender', labelpad = 14,fontsize=15)\n",
        "plt.ylabel('Count', labelpad = 14,fontsize=15)\n",
        "plt.title(\"Count by Gender\", y=1.02, fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZlJMU5aKRuV"
      },
      "source": [
        "major.plot(kind='bar', figsize = (7,6), rot = 0);\n",
        "plt.xlabel('Major', labelpad = 14,fontsize=15)\n",
        "plt.ylabel('Count', labelpad = 14,fontsize=15)\n",
        "plt.title(\"Count by Major Discipline\", y=1.02, fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNq90-x3KRuV"
      },
      "source": [
        "#### Create Bar Graphs for Company Size & Company Type Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "przIdcESzK4o"
      },
      "source": [
        "Create a pie chart using the above variables for company size to show the employee strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyNvqraJKRuV"
      },
      "source": [
        "#Plotting a pie chart\n",
        "#Importing the warnings to supress the deprecation warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "plt.style.use('dark_background')\n",
        "#Setting the size of the Pie chart\n",
        "plt.figure(figsize=[9,7])\n",
        "\n",
        "#Setting the title\n",
        "plt.title(\"Proportion of different company employee strengths\")\n",
        "\n",
        "#Plotting the pie chart\n",
        "train_HR['company_size'].value_counts(normalize=True).plot.pie(autopct='%1.0f%%', textprops={'color':\"b\"})\n",
        "plt.axes().set_ylabel('')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLyZSAgzUCj"
      },
      "source": [
        "According to the above pie chart, it would not be a good idea to completely ignore the missing values given that it constitutes 31% of the total number of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVFYzjOazZ3F"
      },
      "source": [
        "Plot the bar chart using the above variables for company type that is the type that the company falls into."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KrUx7s1zbhz"
      },
      "source": [
        "company_type_count = train_HR['company_type'].value_counts(normalize=True)\n",
        "company_type_frequency = train_HR['company_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZgEao6zd0-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#calculating different values for the bar chart\n",
        "categories = company_type_count.index\n",
        "frequencies = company_type_frequency.values\n",
        "proportions = company_type_count.values * 100\n",
        "\n",
        "#changing the backgroung to black\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "#using the matplotlib to plot the bar chart\n",
        "bar_chart = plt.bar(categories, frequencies, align='edge', width=0.5)\n",
        "\n",
        "#defining the labels\n",
        "plt.xlabel(\"Company Categories\")\n",
        "plt.ylabel(\"No of company categories\")\n",
        "plt.title(\"Proportion of different company categories\")\n",
        "\n",
        "#Declutting the x-axis labels\n",
        "plt.tick_params(axis='x', rotation=60)\n",
        "\n",
        "#Displaying the percentage on the graph\n",
        "i = 0\n",
        "for p in bar_chart:\n",
        "    width = p.get_width()\n",
        "    height = p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    plt.text(x+width/2, y+height*1.01, str(\"{:.2f}\".format(proportions[i]))+'%', ha='center', weight='bold')\n",
        "    i+=1\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahi0kAOizk20"
      },
      "source": [
        "According to the above bar chart, it is evident that 51.24% of the company type that is company category data is ruled by Private Limited organizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHtJZH2OKRuV"
      },
      "source": [
        "#### Calculate Percentage of Job Change Rate for Company Type Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuFYa2WaKRuW"
      },
      "source": [
        "Use groupby to calculate the percentage of job change rate in company type column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MmhLa23KRuW"
      },
      "source": [
        "df1.groupby(\"company_type\")['target'].mean().sort_values() # the percentage of job change rate of elements in column 'company_type'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GL8bcFxKRuW"
      },
      "source": [
        "#### Analysis #1: Create Scatterplot With Top 10 Cities and Job Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGajWqkX0tNh"
      },
      "source": [
        "The scatterplot below is analyzing employees that are looking for a job change and those who aren't, against the city development index. The city ID is used for the purpose of showing cities that have a high development index in general rather than an actual city name. \n",
        "\n",
        "The scatterplot demonstrates that employees located in highly developed cities are not looking for a job change. As the index decreases, there seems to be an increase in employees which are looking for a job change. The cities which are highly developed will tend to have a larger amount of opportunity and therefore employees who are already there won't feel the need to look for a job change. This can be quite problematic for companies in cities with low indexes, as it can lead to lower retention rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejMV8MrRKRuW"
      },
      "source": [
        "top_10_cities = list(aug_traindf['city'].value_counts().head(10).index) # this creates a list of the cities which had the most employees for the scatterplot visualization\n",
        "new_aug_traindf = aug_traindf.loc[aug_traindf['city'].isin(top_10_cities)] # this dataframe contains the top 10 cities with the most employees for the purpose of visualization below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHafH4l86FDa"
      },
      "source": [
        "scatter_city_devlopment_index = sns.catplot(x = 'city', y = 'city_development_index', aspect = 2 , height = 7, data=new_aug_traindf, hue='target', palette = 'bright')# specifies the paramters of the scatterplot and formats it as well\n",
        "scatter_city_devlopment_index.fig.suptitle('EMPLOYEE RETENTION IN TOP 10 EMPLOYEE POPULATED CITIES', y = 1.05) # creates a title that is not overlapping with the scatterplot\n",
        "plt.legend(labels=[\"Looking for a job change\", 'Not looking for job change']) # to make clear what each color represents a legend is created to state the name of what the color repsresents as opposed to a number giving no context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAAnChL80tNj"
      },
      "source": [
        "It seems that a higher city development index causes a lower job change rate from the picture above. However, many columns are not all fully formed by numbers, so we need to do data processing for the columns in train dataset and test dataset in order to find which factor affect the job changing rate the most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTEfaDE7q4Ko"
      },
      "source": [
        "#### Prediction Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu8-Q0QK0tNj"
      },
      "source": [
        "le = LabelEncoder() \n",
        "LGBM = lgbm.LGBMClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C34y8-Xc0tNk"
      },
      "source": [
        "sns.set(rc = {'figure.figsize':(12,6)}) #Setting the default figure size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsiNTrqZ0tNk"
      },
      "source": [
        "#Read files from csv files \n",
        "dfTrain = pd.read_csv(r\"C:\\Users\\linyi\\Desktop\\aug_train.csv\")\n",
        "dfTest = pd.read_csv(r\"C:\\Users\\linyi\\Desktop\\aug_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyyhGb30tNk"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulPeQRXVHkZp"
      },
      "source": [
        "#### Data Cleaning & Processing for Model Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5zLhEX0tNo"
      },
      "source": [
        "We drop the column 'enrollee_id','city' because it serves no purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXh3hJv0tNo"
      },
      "source": [
        "dfTrain = dfTrain.drop(['enrollee_id', 'city'], axis = 1) # drop the columns of enrollee_id and city as they are not useful numeric values for model training\n",
        "dfTest = dfTest.drop(['enrollee_id', 'city'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlMjnQk40tNo"
      },
      "source": [
        "We observe that all the missing values are present in the columns that contain categorical data. Hence, we replace it with the most frequent observation, i.e. Mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEUEGttt0tNo"
      },
      "source": [
        "for column in dfTrain.columns:\n",
        "    dfTrain[column] = dfTrain[column].transform(lambda x: x.fillna(x.mode()[0]))#fill the nan in the training dataset with 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOMtmNr50tNo"
      },
      "source": [
        "for column in dfTest.columns:\n",
        "    dfTest[column] = dfTest[column].transform(lambda x: x.fillna(x.mode()[0]))# fill the nan in the test dataset with 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKOLZHcz0tNp"
      },
      "source": [
        "All null values are imputed now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmDctK5r0tNp"
      },
      "source": [
        "##### Data Processing of the Variables\n",
        "\n",
        "We convert the categorical variables in a format that will be used in our ML models. We map the ordinal variables according to their weight. We transform the nominal variables using the 'LabelEncoder' function to values between 0 and 'n-1'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evdkFZYL0tNr"
      },
      "source": [
        "dfTrain[\"gender\"] = [ 1 if gender == \"Female\" else 0 if gender == \"Male\" else 2 for gender in dfTrain[\"gender\"]] #Female: 1, Male: 0, else: 2\n",
        "dfTest[\"gender\"] = [ 1 if gender == \"Female\" else 0 if gender == \"Male\" else 2 for gender in dfTest[\"gender\"]] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfN57g310tNs"
      },
      "source": [
        "relExpMap = {'No relevent experience': 0, 'Has relevent experience': 1}\n",
        "\n",
        "dfTrain['relevent_experience'] = dfTrain['relevent_experience'].map(relExpMap)\n",
        "dfTest['relevent_experience'] = dfTest['relevent_experience'].map(relExpMap)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2V6Uj5J0tNv"
      },
      "source": [
        "enUniMap = {'no_enrollment': 0, 'Part time course': 1, 'Full time course': 2}\n",
        "\n",
        "dfTrain['enrolled_university'] = dfTrain['enrolled_university'].map(enUniMap)\n",
        "dfTest['enrolled_university'] = dfTest['enrolled_university'].map(enUniMap)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnXZDhzJ0tNy"
      },
      "source": [
        "edLevelMap = {'Primary School': 0, 'High School': 1, 'Graduate': 2, 'Masters': 3, 'Phd': 4}\n",
        "\n",
        "dfTrain['education_level'] = dfTrain['education_level'].map(edLevelMap)\n",
        "dfTest['education_level'] = dfTest['education_level'].map(edLevelMap)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLEyknw0tN1"
      },
      "source": [
        "dfTrain['major_discipline'] = le.fit_transform(dfTrain['major_discipline'])\n",
        "dfTest['major_discipline'] = le.fit_transform(dfTest['major_discipline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRUoj0Mo0tN2"
      },
      "source": [
        "dfTrain['experience'].replace({'>20' : '21', '<1' : '0'}, inplace = True)\n",
        "dfTest['experience'].replace({'>20' : '21', '<1' : '0'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiNNZWF0tN2"
      },
      "source": [
        "dfTrain['experience'] = dfTrain['experience'].astype(int)\n",
        "dfTest['experience'] = dfTest['experience'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsQQ_X7Q0tN3"
      },
      "source": [
        "compSizeMap = {'<10': 1, 'Oct-49': 2, '50-99': 3, '100-500': 4, '500-999': 5, '1000-4999': 6, '5000-9999': 7, '10000+': 8}\n",
        "\n",
        "dfTrain['company_size'] = dfTrain['company_size'].map(compSizeMap)\n",
        "dfTest['company_size'] = dfTest['company_size'].map(compSizeMap)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63bTczpw0tN4"
      },
      "source": [
        "dfTrain['company_size'].fillna(8,inplace=True)\n",
        "dfTest['company_size'].fillna(8,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38scXH5_0tN4"
      },
      "source": [
        "dfTrain.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJHC7K0i0tN6"
      },
      "source": [
        "dfTrain['company_type'] = le.fit_transform(dfTrain['company_type'])\n",
        "dfTest['company_type'] = le.fit_transform(dfTest['company_type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLVp6VEb0tN8"
      },
      "source": [
        "dfTrain['last_new_job'].replace({'>4' : '5', 'never' : '0'}, inplace = True)\n",
        "dfTest['last_new_job'].replace({'>4' : '5', 'never' : '0'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmciTa170tN8"
      },
      "source": [
        "dfTrain['last_new_job'] = dfTrain['last_new_job'].astype(int)\n",
        "dfTest['last_new_job'] = dfTest['last_new_job'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjThKoDL0tN8"
      },
      "source": [
        "dfTrain.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7_9i4wL0tN8"
      },
      "source": [
        "##### Analysis 2: Feature Correlation with the Target Variable\n",
        "1. The factor that affects the job changing rate most will be decided by the correlation of the factors with target.\n",
        "2. The higher the absolute value of correlation means that the factor affects the job changing rate more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlswDKZf0tN8"
      },
      "source": [
        "fig,ax=plt.subplots(figsize=(7,10))\n",
        "sns.heatmap(dfTrain.corr().iloc[-1,:].values.reshape(-1,1).round(2),annot=True,cmap=\"RdBu\")# find the correlations of other columns with target\n",
        "plt.title('Feature Correlation',fontsize=15)\n",
        "ax.set_yticklabels(dfTrain.columns.tolist(),rotation=0)\n",
        "ax.set_xticklabels(['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfZmmAc0tN9"
      },
      "source": [
        "\n",
        "1. We see that the city_development_index has the highest influence on the job rate, meaning the more negatively correlated the target variable with the city_development_index, the more retention takes place, meaning cities with higher index have a better rate of retention.\n",
        "2. This ID, followed by experience and relevant experience, are all  negatively correlated to job changing possibilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMQYBk70tN9"
      },
      "source": [
        "##### Model Training (Includes Analyses 3-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4c7c2L0tN9"
      },
      "source": [
        "We train 5 models:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. LGBM\n",
        "4. Logistic Regression\n",
        "5. KNN \n",
        "\n",
        "We select the one which has the best performance to predict the job changing circumstance of the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4zXApBn0tN9"
      },
      "source": [
        "y = dfTrain['target']\n",
        "X = dfTrain.drop(['target'], axis=1) # the columns other than target column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DZbOP-a0tN9"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state=100, stratify = y) # use 25% of the data as the test-size of training model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQGCbi6m0tN-"
      },
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpyhgzV80tN-"
      },
      "source": [
        "##### 1. Decision Tree (Data analysis 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKzNrgCv0tN-"
      },
      "source": [
        "decisionTree = DecisionTreeClassifier(criterion='gini', random_state = 75 , max_depth = 3, class_weight = 'balanced', min_samples_leaf = 5)# use decisionTreeclassifier\n",
        "decisionTree.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny1Yhf5x0tN-"
      },
      "source": [
        "decisionTreePred = decisionTree.predict(X_val)# predict the result by decisionTree model\n",
        "accuracy_score(y_val, decisionTreePred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNGGwiRs0tN-"
      },
      "source": [
        "It seems the accuracy_score is not high enough.We tune the parameters and use GridSearch to increase the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zogyDo4Z0tN-"
      },
      "source": [
        "model = DecisionTreeClassifier(criterion = 'gini', random_state = 75, class_weight = 'balanced')\n",
        "\n",
        "parameter = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples_leaf' : [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
        "\n",
        "decisionTreeGSCV = GridSearchCV(model, cv = 5, param_grid = parameter, n_jobs = -1)#use grid search to find the optimum parameter\n",
        "\n",
        "decisionTreeGSCV.fit(X_train, y_train)\n",
        "print('Most optimum parameters for the model:- \\n', decisionTreeGSCV.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6shb8l60tN_"
      },
      "source": [
        "decisionTree = DecisionTreeClassifier(criterion='gini', random_state = 75 , max_depth = 2, class_weight = 'balanced', min_samples_leaf = 8)\n",
        "decisionTree.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMpkh_GV0tN_"
      },
      "source": [
        "decisionTreePred = decisionTree.predict(X_val)# predict the result by decisionTree model\n",
        "accuracy_score(y_val, decisionTreePred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUjWuYwt0tN_"
      },
      "source": [
        "##### 2. Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLuvOpJK0tN_"
      },
      "source": [
        "randomForest = RandomForestClassifier(criterion='gini', random_state = 75, max_depth = 8, class_weight = 'balanced', min_samples_leaf = 4, n_estimators = 10)# use randomForest classifier\n",
        "randomForest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6kMNamk0tN_"
      },
      "source": [
        "model = RandomForestClassifier(criterion = 'gini', random_state = 75, class_weight = 'balanced')\n",
        "\n",
        "parameter = {'min_samples_leaf' : [2, 3, 4, 5, 6, 7, 8, 9, 10], 'n_estimators' : [5, 10, 20]}\n",
        "\n",
        "randomforestGSCV = GridSearchCV(model, cv = 10, param_grid = parameter, n_jobs = -1)#use grid search to find the optimum parameter\n",
        "\n",
        "randomforestGSCV.fit(X_train, y_train)\n",
        "print('Most optimum parameters for the model:- \\n', decisionTreeGSCV.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OUNZ1VH0tOA"
      },
      "source": [
        "randomForest = RandomForestClassifier(criterion='gini', random_state = 75, class_weight = 'balanced', min_samples_leaf = 2, n_estimators = 20)\n",
        "randomForest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVjqjMf40tOA"
      },
      "source": [
        "randomForestPred = randomForest.predict(X_val) #predict the result by randomforest model\n",
        "accuracy_score(y_val, randomForestPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeDPtQJM0tOA"
      },
      "source": [
        "##### 3. Lightgbm classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxXMg9v-0tOA"
      },
      "source": [
        "LGBM = lgbm.LGBMClassifier()# Use LGBM classifier\n",
        "LGBM.fit(X_train, y_train)#fit the models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSAbhScO0tOA"
      },
      "source": [
        "LGBMpred=LGBM.predict(X_val)# predict the models by LGBM\n",
        "accuracy_score(y_val, LGBMpred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdmIJkm0tOB"
      },
      "source": [
        "##### 4. Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G32BDX4T0tOB"
      },
      "source": [
        "logregression = LogisticRegression(solver='liblinear')# Use LogisticRegression()\n",
        "logregression.fit(X_train, y_train)\n",
        "predictions = logregression.predict(X_val)#predict the models by logistic regression\n",
        "accuracy_score(predictions, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcJBFGzw0tOB"
      },
      "source": [
        "##### 5. KNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ej0otUe0tOB"
      },
      "source": [
        "kNN = KNeighborsClassifier(n_neighbors=5)# Use KNN classifier\n",
        "kNN.fit(X_train, y_train)\n",
        "predictions1 = kNN.predict(X_val)# predict the models by KNN\n",
        "accuracy_score(y_val, predictions1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgeHEvhS0tOC"
      },
      "source": [
        "We form a dataframe based on the accuracy_score of 5 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feDGVPU30tOC"
      },
      "source": [
        "data = {'models':['Decision Tree','Random Forest','LGBM','Logistic Regression','KNN'],'accuracy_score':[0.7789, 0.7626, 0.7735, 0.7622, 0.7142]}# construct the dataframe with models and their accuracy_score\n",
        "pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7qLUkcY0tOC"
      },
      "source": [
        "Prediction of job changing with the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTVstPT-0tOC"
      },
      "source": [
        "We found that the accuracy score of decisionTree is highest, so use the decisionTree model to predict the 'target' column of dfTest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fez-qEDA0tOD"
      },
      "source": [
        "predictdata=decisionTree.predict(dfTest)\n",
        "predictdata=pd.DataFrame(predictdata)\n",
        "predictdata.rename(columns={0:'target'})\n",
        "predictdata\n",
        "#the result of the target column of dfTest is predicted "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHJwgyN90tOD"
      },
      "source": [
        "predictdata.value_counts()# conduct the number of 0 and 1, 0 means job retention,1 means job change"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS5z8f3y0tOD"
      },
      "source": [
        "numofretention=1728\n",
        "percentageofretention=numofretention/len(predictdata)\n",
        "percentageofretention # The percentage of job retentation is nearly 81%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EomDGzH8-FhU"
      },
      "source": [
        "We may see that the people that doesn't have job change occupys nearly 81%, the decision tree model predicts the result well \n"
      ]
    }
  ]
}
